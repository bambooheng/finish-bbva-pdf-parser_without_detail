"""
Gemini API client for structured markdown parsing
"""
import json
import time
import requests
from typing import Dict, Any
from config import config


class GeminiClient:
    """Client for interacting with Gemini API for markdown parsing"""
    
    def __init__(self):
        """Initialize Gemini API client"""
        config.validate()
        self.api_key = config.gemini_api_key
        self.model_name = config.gemini_model
        self.base_url = config.base_url
        
        print(f"âœ“ Geminiå®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œæ¨¡å‹: {self.model_name}")
    
    def parse_markdown_to_json(self, markdown_content: str, source_file: str) -> Dict[str, Any]:
        """
        Parse markdown content to structured JSON using Gemini
        
        Args:
            markdown_content: The markdown content to parse
            source_file: Original source file name
            
        Returns:
            Dictionary containing structured data
        """
        print(f"â†’ æ­£åœ¨ä½¿ç”¨Geminiè§£æMarkdown...")
        print(f"  å†…å®¹é•¿åº¦: {len(markdown_content)} å­—ç¬¦")
        
        # Split by pages for parallel processing
        import re
        page_pattern = r'(---\s*\n\n## Page \d+\s*\n\n)'
        parts = re.split(page_pattern, markdown_content)
        
        # Reconstruct pages
        pages = []
        current_content = ""
        for i, part in enumerate(parts):
            if re.match(r'---\s*\n\n## Page \d+\s*\n\n', part):
                if current_content.strip():
                    pages.append(current_content)
                current_content = part
            else:
                current_content += part
        if current_content.strip():
            pages.append(current_content)
        
        # If document is small or single page, use single request
        if len(pages) <= 1 or len(markdown_content) < 15000:
            return self._parse_single(markdown_content)
        
        print(f"  ğŸ“„ åˆ†é¡µå¹¶è¡Œå¤„ç†: {len(pages)} é¡µ")
        
        # Parallel processing
        from concurrent.futures import ThreadPoolExecutor, as_completed
        page_results = [None] * len(pages)
        
        start_time = time.time()
        
        # ä½¿ç”¨é…ç½®çš„å¹¶å‘æ•°ï¼ˆé»˜è®¤ä¸º5ï¼‰
        with ThreadPoolExecutor(max_workers=min(config.max_workers, len(pages))) as executor:
            future_to_idx = {
                executor.submit(self._parse_single_page, page, idx): idx 
                for idx, page in enumerate(pages)
            }
            
            completed = 0
            for future in as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    page_results[idx] = future.result()
                    completed += 1
                    print(f"  âœ“ é¡µé¢ {idx + 1}/{len(pages)} è§£æå®Œæˆ")
                except Exception as e:
                    print(f"  âœ— é¡µé¢ {idx + 1} è§£æå¤±è´¥: {str(e)}")
                    page_results[idx] = {"error": str(e)}
        
        elapsed = time.time() - start_time
        print(f"âœ“ å¹¶è¡Œè§£æå®Œæˆ ({elapsed:.2f}s)")
        
        # Merge results
        return self._merge_page_results(page_results)
    
    def _parse_single(self, markdown_content: str) -> Dict[str, Any]:
        """Parse entire markdown as single request"""
        prompt = self._build_prompt(markdown_content)
        
        try:
            start_time = time.time()
            response_text = self._call_gemini(prompt)
            elapsed = time.time() - start_time
            
            print(f"âœ“ æ”¶åˆ°Geminiå“åº” ({elapsed:.2f}s)")
            
            result = self._extract_json(response_text)
            return result
            
        except Exception as e:
            print(f"âœ— Gemini APIè°ƒç”¨é”™è¯¯: {str(e)}")
            raise
    
    def _parse_single_page(self, page_content: str, page_idx: int) -> Dict[str, Any]:
        """Parse a single page"""
        # å·²ç§»é™¤å»¶è¿Ÿä»¥æé«˜æ€§èƒ½
        prompt = self._build_prompt(page_content)
        response_text = self._call_gemini(prompt)
        return self._extract_json(response_text)
    
    def _merge_page_results(self, page_results: list) -> Dict[str, Any]:
        """Merge results from multiple pages into single document"""
        merged = {
            "document_type": "unknown",
            "page_metadata": [],
            "content": {
                "sections": []
            }
        }
        
        for idx, result in enumerate(page_results):
            if result is None or "error" in result:
                continue
            
            # Get document type from first valid result
            if merged["document_type"] == "unknown" and result.get("document_type"):
                merged["document_type"] = result["document_type"]
            
            # Merge page metadata
            if "page_metadata" in result:
                merged["page_metadata"].extend(result["page_metadata"])
            
            # Merge sections
            if "content" in result and "sections" in result["content"]:
                sections = result["content"]["sections"]
                # Standardize keys for transaction sections
                # Process sections for standardization
                for section in sections:
                    data = section.get("data")
                    if isinstance(data, list):
                        # Heuristic: If list contains dicts with transaction-like keys, apply standardization
                        if data and isinstance(data[0], dict):
                            keys = set(k.upper() for k in data[0].keys())
                            if any(k in keys for k in ["DESCRIPCIÃ“N", "DESCRIPCION", "OPER", "FECHA OPER"]):
                                section["data"] = self._standardize_transaction_keys(data)
                    elif isinstance(data, dict):
                        # Clean summary keys (e.g. "Label 8": "Amount" -> "Label": "8 Amount")
                        section["data"] = self._clean_summary_keys(data)
                        
                merged["content"]["sections"].extend(sections)
        
        return merged

    def _standardize_transaction_keys(self, data: list) -> list:
        """Standardize keys in transaction records"""
        if not isinstance(data, list):
            return data
            
        standardized_data = []
        # Key mapping (Synonym -> Standard)
        key_map = {
            "FECHA OPER": "OPER",
            "FECHA LIQ": "LIQ",
            "DESCRIPCION": "DESCRIPCIÃ“N",
            "REF.": "REFERENCIA",
            "SALDO OPERACION": "OPERACIÃ“N",
            "SALDO LIQUIDACION": "LIQUIDACIÃ“N",
            "OPERACION": "OPERACIÃ“N",
            "LIQUIDACION": "LIQUIDACIÃ“N",
            "SALDO": "OPERACIÃ“N" # Map generic SALDO to OPERACIÃ“N as default balance
        }
        
        # Required keys that must exist (value will be null if missing)
        required_keys = ["OPERACIÃ“N", "LIQUIDACIÃ“N", "REFERENCIA"]
        
        for record in data:
            if not isinstance(record, dict):
                standardized_data.append(record)
                continue
                
            new_record = {}
            for k, v in record.items():
                upper_k = k.upper().strip()
                # Apply mapping or use original key
                standard_k = key_map.get(upper_k, k)
                new_record[standard_k] = v
            
            # Ensure required keys exist and apply fallback logic
            if new_record.get("LIQUIDACIÃ“N") is None:
                if new_record.get("OPERACIÃ“N") is not None:
                    new_record["LIQUIDACIÃ“N"] = new_record["OPERACIÃ“N"]
                else:
                    new_record["LIQUIDACIÃ“N"] = None
            
            if new_record.get("OPERACIÃ“N") is None:
                new_record["OPERACIÃ“N"] = None

            # STRICT PARSING: Remove any keys that are not allowed
            # This prevents hallucinated fields like "SALDO DIARIO"
            allowed_keys = {
                "OPER", "LIQ", "DESCRIPCIÃ“N", "REFERENCIA", 
                "CARGOS", "ABONOS", "OPERACIÃ“N", "LIQUIDACIÃ“N"
            }
            final_record = {k: v for k, v in new_record.items() if k in allowed_keys}
            
            # HEURISTIC CORRECTION: Fix swapped columns based on keywords
            final_record = self._apply_heuristic_correction(final_record)
            
            standardized_data.append(final_record)
            
        return standardized_data

    def _apply_heuristic_correction(self, record: dict) -> dict:
        """
        Apply heuristic rules to correct column swaps (Cargos vs Abonos)
        based on description keywords.
        """
        desc = record.get("DESCRIPCIÃ“N", "").upper()
        
        # Keywords that strongly imply CARGOS (Withdrawals/Payments)
        # Removed broad "PAGO" to avoid false positives like "PAGO DE NOMINA"
        cargo_keywords = [
            "COMPRA", "RETIRO", "ENVIADO", "COMISION", 
            "CGO", "CARGO", "INTERES", "PAGO DE SERVICIOS",
            "PAGO CUENTA DE TERCERO", "TRASPASO A TERCEROS",
            "CHEQUE PAGADO", "MEMBRESIA", "SUSCRIPCION"
        ]
        
        # Keywords that strongly imply ABONOS (Deposits/Credits)
        abono_keywords = [
            "ABONO", "DEPOSITO", "RECIBIDO", "NOMINA", "DEVOLUCION", 
            "REEMBOLSO", "TRASPASO DE TERCEROS", "PAGO DE NOMINA",
            "TRANSFERENCIA RECIBIDA"
        ]
        
        cargos = record.get("CARGOS")
        abonos = record.get("ABONOS")
        
        # Logic 1: Implicit CARGO found in ABONOS
        if any(kw in desc for kw in cargo_keywords):
            # Special Case: "PAGO DE NOMINA" contains "PAGO" but is an ABONO
            if "NOMINA" in desc:
                pass # Do not swap if it's payroll
            elif not cargos and abonos:
                # print(f"  ğŸ”§ Auto-Correcting: Moved '{abonos}' from ABONOS to CARGOS based on '{desc[:20]}...'")
                record["CARGOS"] = abonos
                record["ABONOS"] = None
                
        # Logic 2: Implicit ABONO found in CARGOS
        elif any(kw in desc for kw in abono_keywords):
            if not abonos and cargos:
                # print(f"  ğŸ”§ Auto-Correcting: Moved '{cargos}' from CARGOS to ABONOS based on '{desc[:20]}...'")
                record["ABONOS"] = cargos
                record["CARGOS"] = None
        
        return record
            
        return standardized_data

    def _clean_summary_keys(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Clean summary keys where count is merged into the key.
        Example: "DepÃ³sitos / Abonos (+) 8": "22,400.01" -> "DepÃ³sitos / Abonos (+)": "8 22,400.01"
        """
        if not isinstance(data, dict):
            return data
            
        cleaned_data = {}
        import re
        
        for k, v in data.items():
            # Match keys ending with space + number (e.g. "Label 123")
            match = re.search(r'^(.*?)\s+(\d+)$', k)
            if match:
                clean_key = match.group(1).strip()
                count = match.group(2)
                
                # Combine count and value in the value string
                if isinstance(v, str):
                    new_value = f"{count} {v}"
                else:
                    new_value = v # Fallback if value is not string
                
                cleaned_data[clean_key] = new_value
            else:
                cleaned_data[k] = v
                
        return cleaned_data
    
    def _call_gemini(self, prompt: str, retry_count: int = 0) -> str:
        """Call Gemini API using HTTP requests with retry logic"""
        import time
        
        url = f"{self.base_url}/{self.model_name}:generateContent?key={self.api_key}"
        headers = {"Content-Type": "application/json"}
        
        data = {
            "contents": [
                {
                    "role": "user",
                    "parts": [
                        {"text": prompt}
                    ]
                }
            ],
            "generationConfig": {
                "temperature": config.temperature,
                "maxOutputTokens": config.max_output_tokens
            }
        }
        
        max_retries = 3
        
        try:
            # Increase timeout for large documents
            response = requests.post(url, headers=headers, json=data, timeout=600)
            
            if response.status_code == 200:
                res_json = response.json()
                candidate = res_json["candidates"][0]
                parts = candidate.get("content", {}).get("parts", [])
                
                # Check for truncation
                finish_reason = candidate.get("finishReason", "")
                if finish_reason == "MAX_TOKENS" and retry_count < 2:
                    print(f"  âš  å“åº”è¢«æˆªæ–­ï¼Œæ­£åœ¨é‡è¯• ({retry_count + 1}/2)...")
                    return self._call_gemini(prompt, retry_count + 1)
                
                full_response = ""
                for part in parts:
                    if "text" in part:
                        full_response += part["text"]
                    elif "thought" in part:
                        print(f"  ğŸ’­ Geminiæ€è€ƒè¿‡ç¨‹å·²æ£€æµ‹")
                
                return full_response
            else:
                error_msg = f"{response.status_code} - {response.text}"
                raise Exception(error_msg)
                
        except requests.exceptions.Timeout:
            if retry_count < max_retries:
                wait_time = (retry_count + 1) * 10
                print(f"  âš  è¯·æ±‚è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯• ({retry_count + 1}/{max_retries})...")
                time.sleep(wait_time)
                return self._call_gemini(prompt, retry_count + 1)
            raise Exception("è¯·æ±‚è¶…æ—¶ï¼ˆ600ç§’ï¼‰ï¼Œå·²é‡è¯•3æ¬¡")
        except (requests.exceptions.ConnectionError, requests.exceptions.RequestException) as e:
            if retry_count < max_retries:
                wait_time = (retry_count + 1) * 10
                print(f"  âš  è¿æ¥é”™è¯¯ï¼Œ{wait_time}ç§’åé‡è¯• ({retry_count + 1}/{max_retries})...")
                time.sleep(wait_time)
                return self._call_gemini(prompt, retry_count + 1)
            raise Exception(f"è¯·æ±‚å¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰: {str(e)}")
    
    def _build_prompt(self, markdown_content: str) -> str:
        """Build the prompt for Gemini to parse markdown"""
        prompt = f"""STRICT JSON OUTPUT ONLY. DO NOT START WITH "Here is the JSON" OR ANY OTHER TEXT. START DIRECTLY WITH "{{".
DO NOT USE MARKDOWN FORMATTING like ```json. JUST RAW JSON.

CRITICAL: Output ONLY valid JSON. No explanations, no thinking, no markdown formatting.
Start your response with {{ and end with }}. Nothing else.

ABSOLUTE PROHIBITION - READ CAREFULLY:
1. Do NOT add fields that don't exist in the original document
2. Do NOT split values into sub-objects
3. Keep values EXACTLY as they appear

WRONG (adding non-existent fields like Cantidad/Importe):
"DepÃ³sitos / Abonos (+)": {{"Cantidad": "5", "Importe": "233,768.72"}}

CORRECT (keeping original format):
"DepÃ³sitos / Abonos (+)": "5 233,768.72"

ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç»“æ„åŒ–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†Markdownæ–‡æ¡£è½¬æ¢ä¸ºç»“æ„åŒ–JSONï¼Œç¡®ä¿ç»å¯¹çš„é›¶ä¿¡æ¯ä¸¢å¤±ã€‚

# æ ¸å¿ƒåŸåˆ™ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

## 1. å®Œæ•´æ€§åŸåˆ™ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
**ç»å¯¹ç¦æ­¢æˆªæ–­ã€çœç•¥ã€ç®€åŒ–ä»»ä½•ä¿¡æ¯**

- âœ… æå–æ¯ä¸€ä¸ªå­—ç¬¦ã€æ¯ä¸€ä¸ªç©ºæ ¼ã€æ¯ä¸€ä¸ªç¬¦å·
- âœ… å¤šè¡Œå†…å®¹å¿…é¡»å®Œæ•´åˆå¹¶ï¼ˆç”¨ç©ºæ ¼è¿æ¥ï¼‰
- âœ… è¡¨æ ¼çš„æ¯ä¸€ä¸ªå•å…ƒæ ¼éƒ½è¦å®Œæ•´æå–
- âœ… è·¨é¡µå†…å®¹å¿…é¡»å®Œæ•´åˆå¹¶
- âŒ ç»å¯¹ä¸èƒ½å› ä¸ºå†…å®¹é•¿å°±æˆªæ–­
- âŒ ç»å¯¹ä¸èƒ½çœç•¥ä»»ä½•ä¿¡æ¯

## 2. å­—æ®µåç²¾ç¡®å¤åˆ¶åŸåˆ™ï¼ˆå…³é”®ï¼ï¼‰
**å­—æ®µåå¿…é¡»100%ç²¾ç¡®å¤åˆ¶åŸæ–‡æ¡£ä¸­çš„æ–‡å­—**

âŒ ç¦æ­¢çš„æ“ä½œï¼š
- ä¸è¦å°†"Saldo Promedio"æ”¹æˆ"saldo_promedio"
- ä¸è¦å°†"DÃ­as del Periodo"æ”¹æˆ"dias_del_periodo"  
- ä¸è¦ç§»é™¤ç©ºæ ¼ã€ç‰¹æ®Šå­—ç¬¦ï¼ˆ+ã€-ã€%ã€/ç­‰ï¼‰
- ä¸è¦æ”¹å˜å¤§å°å†™

âœ… æ­£ç¡®åšæ³•ï¼š
- "DepÃ³sitos / Abonos (+)" â†’ ä½¿ç”¨ "DepÃ³sitos / Abonos (+)" ä½œä¸ºå­—æ®µå
- "ISR Retenido (-)" â†’ ä½¿ç”¨ "ISR Retenido (-)" ä½œä¸ºå­—æ®µå
- "Tasa Bruta Anual %" â†’ ä½¿ç”¨ "Tasa Bruta Anual %" ä½œä¸ºå­—æ®µå

## 3. ç¦æ­¢æ•°æ®é‡æ„åŸåˆ™ï¼ˆå…³é”®ï¼ï¼‰
**å€¼å¿…é¡»ä¿æŒåŸæ ·ï¼Œä¸èƒ½æ·»åŠ å­å­—æ®µæˆ–è§£é‡Š**

âŒ ç¦æ­¢çš„æ“ä½œï¼š
- ä¸è¦å°† "5 233,768.72" æ‹†åˆ†æˆ {{"Cantidad": "5", "Importe": "233,768.72"}}
- ä¸è¦æ·»åŠ åŸæ–‡æ¡£ä¸­ä¸å­˜åœ¨çš„å­—æ®µåï¼ˆå¦‚Cantidadã€Importeï¼‰
- ä¸è¦å¯¹æ•°æ®è¿›è¡Œä»»ä½•è§£é‡Šã€æ€»ç»“æˆ–é‡æ–°ç»„ç»‡

âœ… æ­£ç¡®åšæ³•ï¼š
- åŸæ–‡æ˜¾ç¤º "DepÃ³sitos / Abonos (+): 5 233,768.72"
- è¾“å‡ºåº”ä¸º: "DepÃ³sitos / Abonos (+)": "5 233,768.72"
- ä¿æŒå€¼çš„åŸå§‹æ ¼å¼ï¼Œä¸åšä»»ä½•æ‹†åˆ†

**ç¤ºä¾‹å¯¹æ¯”ï¼š**
åŸæ–‡ï¼šDepÃ³sitos / Abonos (+)  5  233,768.72

âŒ é”™è¯¯è¾“å‡ºï¼ˆæ·»åŠ äº†ä¸å­˜åœ¨çš„å­å­—æ®µï¼‰ï¼š
"DepÃ³sitos / Abonos (+)": {{"Cantidad": "5", "Importe": "233,768.72"}}

âœ… æ­£ç¡®è¾“å‡ºï¼ˆä¿æŒåŸæ ·ï¼‰ï¼š
"DepÃ³sitos / Abonos (+)": "5 233,768.72"

## 4. é€šç”¨æ€§åŸåˆ™
**ç³»ç»Ÿå¿…é¡»èƒ½å¤„ç†ä»»æ„ç±»å‹çš„æ–‡æ¡£**

- æ ¹æ®å®é™…å†…å®¹åŠ¨æ€è¯†åˆ«æ–‡æ¡£ç±»å‹ï¼ˆdocument_typeï¼‰
- æ ¹æ®è¡¨æ ¼çš„å®é™…åˆ—å/è¡Œæ ‡é¢˜åˆ›å»ºå­—æ®µ
- ä¸è¦å‡è®¾å›ºå®šçš„è¡¨æ ¼ç»“æ„
- è‡ªåŠ¨é€‚åº”ä¸åŒçš„æ–‡æ¡£æ ¼å¼
- **ä¿æŒåŸå§‹è¯­è¨€ï¼Œä¸è¦ç¿»è¯‘ä»»ä½•å†…å®¹**

## 5. è¡¨æ ¼å’Œæ‘˜è¦åŒºåŸŸå¤„ç†
**ä¿æŒåŸå§‹ç»“æ„å’Œæ ‡ç­¾**

- ä½¿ç”¨æ–‡æ¡£ä¸­å®é™…æ˜¾ç¤ºçš„æ–‡å­—ä½œä¸ºJSONå­—æ®µå
- ä¿æŒåŸå§‹è¯­è¨€å’Œæ ¼å¼
- å¯¹äºé”®å€¼å¯¹å½¢å¼çš„æ‘˜è¦ä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹æ ‡ç­¾ä½œä¸ºå­—æ®µå
- å€¼å¿…é¡»ä¿æŒåŸå§‹æ ¼å¼ï¼Œä¸åšæ‹†åˆ†æˆ–é‡æ„
- å¦‚"Saldo Anterior: 12,383.20" â†’ {{"Saldo Anterior": "12,383.20"}}

## 6. é¡µé¢å…ƒæ•°æ®æå–
**å¦‚æœæ–‡æ¡£åŒ…å«é¡µé¢ä¿¡æ¯ï¼Œè¿›è¡Œæå–**

æå–å¯èƒ½å­˜åœ¨çš„ï¼š
- è´¦å·ã€å®¢æˆ·å·ã€æ–‡æ¡£ç¼–å·ç­‰æ ‡è¯†ç¬¦
- é¡µç ä¿¡æ¯
- æ—¥æœŸèŒƒå›´ç­‰

# JSONè¾“å‡ºæ ¼å¼

```json
{{
  "document_type": "æ ¹æ®å†…å®¹è‡ªåŠ¨è¯†åˆ«ï¼šbank_statement, invoice, report, contract, formç­‰",
  "page_metadata": [
    {{"page": 1, ...å…¶ä»–é¡µé¢çº§ä¿¡æ¯...}}
  ],
  "content": {{
    "sections": [
      {{
        "section_type": "æ ¹æ®å†…å®¹è¯†åˆ«ï¼šheader, summary, transactions, table_dataç­‰",
        "title": "è¯¥åˆ†åŒºçš„æ ‡é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰",
        "data": {{
          // ä½¿ç”¨åŸå§‹å­—æ®µåï¼Œä¿æŒåŸå§‹è¯­è¨€
        }}
      }}
    ]
  }}
}}
```

# é“¶è¡Œå¯¹è´¦å•ç‰¹æ®Šå¤„ç†ï¼ˆå¦‚æœæ£€æµ‹åˆ°ï¼‰

å¦‚æœæ–‡æ¡£æ˜¯é“¶è¡Œå¯¹è´¦å•ï¼Œä¸”è¡¨æ ¼æœ‰è¿™äº›åˆ—ï¼š
- OPER, LIQ, DESCRIPCION, REFERENCIA, CARGOS, ABONOS, OPERACION, LIQUIDACION

åˆ™ï¼š
- ä¿æŒè¿™äº›åŸå§‹åˆ—åä½œä¸ºJSONå­—æ®µå
- **DESCRIPCIÃ“Nåˆ—è§„åˆ™**ï¼ˆé‡è¦ï¼è§‚å¯ŸPDFè¡¨æ ¼çš„è§†è§‰æ’åˆ—ï¼‰ï¼š
  - DESCRIPCIÃ“Nåˆ—åªåŒ…å«äº¤æ˜“æè¿°/å•†æˆ·åç§°
  - ä¾‹å¦‚ï¼š`"GASOL SERV COLIMA2"`, `"AUTOZONE 7740"`, `"RETIRO CAJERO AUTOMATICO"`
  - ä¸è¦åœ¨DESCRIPCIÃ“Nä¸­åŒ…å«æ˜Ÿå·(******)æˆ–RFCä¿¡æ¯
- **REFERENCIAåˆ—è§„åˆ™**ï¼ˆå…³é”®ï¼åŠ¨æ€è¯†åˆ«ï¼Œä¸è¦ç¡¬ç¼–ç ï¼‰ï¼š
  - REFERENCIAåˆ—åŒ…å«å¡å·åå‡ ä½ï¼ˆä»¥æ˜Ÿå·******å¼€å¤´ï¼‰å’Œå‚è€ƒä¿¡æ¯
  - å…¸å‹æ ¼å¼ï¼š`"******6275 RFC: SCO 7312133D6 10:14 AUT: 202329"`
  - å¯èƒ½åŒ…å«çš„å†…å®¹ï¼š
    - å¡å·åå‡ ä½ï¼š`******6275`
    - ç¨å·ï¼š`RFC: xxx`
    - æˆæƒç ï¼š`AUT: xxx`
    - æ—¶é—´ï¼š`10:14`
    - æµæ°´å·ï¼š`FOLIO:xxx`
    - è´¦å·/å‚è€ƒå·
  - å¦‚æœå•å…ƒæ ¼ä»¥æ˜Ÿå·ï¼ˆ******ï¼‰å¼€å¤´ï¼Œè¯¥å†…å®¹å±äºREFERENCIAåˆ—
  - REFERENCIAå¯èƒ½ä¸ºç©ºï¼ˆå¦‚SPEIè½¬è´¦ç­‰æ— å¡äº¤æ˜“ï¼‰
- **ä¸è¦å°†DESCRIPCIÃ“Nçš„å†…å®¹æ”¾å…¥REFERENCIA**
- **ä¸¥æ ¼ç¦æ­¢æ·»åŠ ä¸å­˜åœ¨çš„å­—æ®µï¼ˆABSOLUTELY FORBIDDENï¼‰**:
  - **ä¸¥ç¦**ç»è¿‡è®¡ç®—ã€æ¨å¯¼æˆ–æ€»ç»“æ·»åŠ ä»»ä½•åŸå§‹æ–‡æ¡£ä¸­ä¸å­˜åœ¨çš„åˆ—ã€‚
  - **ä¸¥ç¦**æ·»åŠ  "SALDO DIARIO"ã€"TOTAL"ã€"SUBTOTAL" ç­‰åŸå§‹è¡¨æ ¼ä¸­æ²¡æœ‰çš„å­—æ®µã€‚
  - å¦‚æœåŸå§‹è¡¨æ ¼åªæœ‰ "SALDO" ä¸€åˆ—ï¼Œå°±åªè¾“å‡º "SALDO"ï¼Œç»å¯¹ä¸è¦è‡ªå·±æ‹†åˆ†æˆ "SALDO DIARIO"ã€‚
  - **åŸåˆ™**ï¼šåŸæ–‡æ¡£æœ‰ä»€ä¹ˆå­—æ®µå°±è¾“å‡ºä»€ä¹ˆå­—æ®µï¼Œä¸åšä»»ä½•é€»è¾‘åˆ¤æ–­ï¼Œä¸åšçº æ­£ï¼Œä¸åšå·²å­˜åœ¨çš„æ€»ç»“ã€‚
  - å³ä½¿å‘ç°æ•°æ®ä¸å¹³è¡¡æˆ–çœ‹èµ·æ¥æœ‰é”™ï¼Œä¹Ÿå¿…é¡»**æŒ‰åŸæ ·æå–**ã€‚

# è·¨é¡µè¡¨æ ¼è¡Œåˆå¹¶è§„åˆ™ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼ï¼‰

æ–‡æ¡£ä¸­åŒ…å«å¤šä¸ªé¡µé¢ï¼ˆç”¨ "---" å’Œ "## Page X" åˆ†éš”ï¼‰ã€‚å½“è¡¨æ ¼è·¨é¡µæ—¶ï¼Œå¿…é¡»æ­£ç¡®åˆå¹¶ã€‚

**å¦‚ä½•è¯†åˆ«éœ€è¦åˆå¹¶çš„è¡Œï¼š**

æ–¹æ³•1ï¼šæ£€æŸ¥æ ‡è®°
- å¦‚æœçœ‹åˆ° `<!-- ROW_CONTINUES_NEXT_PAGE -->` å’Œ `<!-- ROW_CONTINUED_FROM_PREV_PAGE -->`
- è¿™ä¸¤ä¸ªæ ‡è®°ä¹‹é—´çš„å†…å®¹éœ€è¦åˆå¹¶ä¸ºä¸€æ¡è®°å½•

æ–¹æ³•2ï¼šæ£€æŸ¥æ•°æ®å®Œæ•´æ€§
- é¡µé¢æœ«å°¾çš„è¡¨æ ¼è¡Œï¼šå¦‚æœæ‰€æœ‰é‡‘é¢åˆ—ï¼ˆCARGOS/ABONOS/OPERACION/LIQUIDACIONï¼‰éƒ½ä¸ºç©ºï¼Œè¯¥è¡Œä¸å®Œæ•´
- ä¸‹ä¸€é¡µå¼€å¤´çš„è¡¨æ ¼è¡Œï¼šå¦‚æœæ—¥æœŸåˆ—ï¼ˆOPER/LIQï¼‰ä¸ºç©ºï¼Œè¯¥è¡Œæ˜¯å‰ä¸€è¡Œçš„å»¶ç»­

**åˆå¹¶è§„åˆ™ï¼ˆå¿…é¡»ä¸¥æ ¼æ‰§è¡Œï¼‰ï¼š**
1. æ‰¾åˆ°é¡µé¢Næœ«å°¾çš„ä¸å®Œæ•´è¡Œï¼ˆç¼ºå°‘é‡‘é¢ï¼‰
2. æ‰¾åˆ°é¡µé¢N+1å¼€å¤´çš„å»¶ç»­è¡Œï¼ˆç¼ºå°‘æ—¥æœŸï¼‰
3. å°†ä¸¤è€…åˆå¹¶ä¸ºä¸€æ¡å®Œæ•´è®°å½•ï¼š
   - æ—¥æœŸå–è‡ªä¸å®Œæ•´è¡Œ
   - DESCRIPCIONåˆå¹¶ä¸¤è¡Œçš„å†…å®¹ï¼ˆç”¨ç©ºæ ¼è¿æ¥ï¼‰
   - é‡‘é¢å–è‡ªå»¶ç»­è¡Œ

**ç¤ºä¾‹åœºæ™¯ï¼š**
```
é¡µé¢2æœ«å°¾ï¼ˆä¸å®Œæ•´è¡Œ - æ— é‡‘é¢ï¼‰ï¼š
OPER: 26/Jun, LIQ: 26/Jun, DESCRIPCION: SPEI RECIBIDOSANTANDER 5292262...

é¡µé¢3å¼€å¤´ï¼ˆå»¶ç»­è¡Œ - æ— æ—¥æœŸï¼‰ï¼š
OPER: ç©º, LIQ: ç©º, DESCRIPCION: 20250626400140BET0000452922620 AS INTERMODAL..., ABONOS: 15,000.00

é¡µé¢3ç¬¬äºŒè¡Œï¼ˆæ–°è®°å½• - æœ‰æ—¥æœŸï¼‰ï¼š
OPER: 26/Jun, LIQ: 26/Jun, DESCRIPCION: PAGO TARJETA DE CREDITO..., CARGOS: 5,000.00
```

**æ­£ç¡®ç»“æœï¼š**
è®°å½•1: SPEI RECIBIDOSANTANDER... + 20250626400140BET... = åˆå¹¶ä¸ºä¸€æ¡ (ABONOS: 15,000.00)
è®°å½•2: PAGO TARJETA DE CREDITO... = å•ç‹¬ä¸€æ¡ (CARGOS: 5,000.00)

**é”™è¯¯ç»“æœï¼ˆå¿…é¡»é¿å…ï¼‰ï¼š**
å°† "20250626400140BET..." å½’å…¥ "PAGO TARJETA DE CREDITO" è®°å½•

**æ£€æµ‹å…³é”®ç‚¹ï¼š**
- å»¶ç»­è¡Œçš„ç¬¬ä¸€åˆ—ï¼ˆæ—¥æœŸï¼‰é€šå¸¸ä¸ºç©ºæˆ–ä»…å«ç©ºç™½
- å¦‚æœä¸‹ä¸€é¡µçš„ç¬¬ä¸€æ¡è®°å½•æ²¡æœ‰æ—¥æœŸï¼Œå®ƒä¸€å®šæ˜¯ä¸Šä¸€é¡µè®°å½•çš„å»¶ç»­
**é”™è¯¯å¤„ç†ï¼ˆå¿…é¡»é¿å…ï¼‰ï¼š**
ä¸è¦å°†å»¶ç»­å†…å®¹å½’å…¥ä¸‹ä¸€æ¡è®°å½•ã€‚å¦‚æœä¸‹ä¸€é¡µå¼€å¤´çš„å†…å®¹ç¼ºå°‘æ—¥æœŸï¼Œå®ƒä¸€å®šæ˜¯ä¸Šä¸€æ¡è®°å½•çš„å»¶ç»­ã€‚

# éªŒè¯æ£€æŸ¥æ¸…å•

ç”ŸæˆJSONå‰éªŒè¯ï¼š
1. â˜‘ æ˜¯å¦æœ‰ä»»ä½•å†…å®¹è¢«æˆªæ–­ï¼Ÿ
2. â˜‘ è¡¨æ ¼åˆ—åæ˜¯å¦ä½¿ç”¨åŸå§‹åç§°ï¼Ÿ
3. â˜‘ è·¨é¡µå†…å®¹æ˜¯å¦å®Œæ•´åˆå¹¶ï¼Ÿ
4. â˜‘ æ¯é¡µå…ƒæ•°æ®æ˜¯å¦æå–ï¼Ÿ

# æºæ–‡æ¡£

{markdown_content}

# è¾“å‡ºè¦æ±‚

1. **åªè¿”å›JSON**ï¼Œä¸è¦ä»»ä½•è§£é‡Šæ–‡å­—
2. **ç¡®ä¿JSONæ ¼å¼æ­£ç¡®**ä¸”å¯è§£æ
3. **ç»å¯¹é›¶ä¿¡æ¯ä¸¢å¤±**
4. **ä¿æŒåŸå§‹å­—æ®µåå’Œè¯­è¨€**

å¼€å§‹è½¬æ¢ï¼š
"""
        return prompt
    
    def _sanitize_json_text(self, text: str) -> str:
        """Remove invalid control characters from JSON text"""
        import re
        # Remove control characters except for \t, \n, \r which are valid in some contexts
        # In JSON strings, these should be escaped. Remove unescaped ones.
        
        # First, find all string literals and sanitize them
        def clean_string_content(match):
            content = match.group(0)
            # Replace unescaped control characters within strings
            # Keep \n, \r, \t if they appear as escaped sequences
            cleaned = ""
            i = 0
            while i < len(content):
                char = content[i]
                if char == '\\' and i + 1 < len(content):
                    # Keep escaped sequences
                    cleaned += content[i:i+2]
                    i += 2
                elif ord(char) < 32 and char not in '\t':
                    # Replace control characters with space
                    cleaned += ' '
                    i += 1
                else:
                    cleaned += char
                    i += 1
            return cleaned
        
        # Simple approach: replace all control characters except those in valid escape sequences
        result = []
        in_string = False
        escape_next = False
        
        for char in text:
            if escape_next:
                result.append(char)
                escape_next = False
            elif char == '\\' and in_string:
                result.append(char)
                escape_next = True
            elif char == '"' and not escape_next:
                result.append(char)
                in_string = not in_string
            elif in_string and ord(char) < 32 and char not in '\t':
                # Replace control characters with space inside strings
                result.append(' ')
            else:
                result.append(char)
        
        return ''.join(result)
    
    def _extract_json(self, response_text: str) -> Dict[str, Any]:
        """Extract and parse JSON from Gemini response"""
        text = response_text.strip()
        import re
        
        # Strategy 1: Find Markdown Code Block (Highest Confidence)
        # Check for ```json or just ``` blocks
        code_block_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
        matches = re.findall(code_block_pattern, text, re.DOTALL)
        if matches:
            # Try matches, preferring the largest one or the last one
            for match in reversed(matches):
                try:
                    json_text = self._sanitize_json_text(match)
                    result = json.loads(json_text)
                    print(f"âœ“ JSONè§£ææˆåŠŸ (ä»ä»£ç å—æå–)")
                    return result
                except:
                    continue

        # Strategy 2: Find last } and matching { (Good for "thinking first, json last")
        brace_end = text.rfind('}')
        if brace_end != -1:
            brace_count = 0
            for i in range(brace_end, -1, -1):
                if text[i] == '}':
                    brace_count += 1
                elif text[i] == '{':
                    brace_count -= 1
                    if brace_count == 0:
                        try:
                            json_text = text[i:brace_end+1]
                            json_text = self._sanitize_json_text(json_text)
                            result = json.loads(json_text)
                            print(f"âœ“ JSONè§£ææˆåŠŸ (ä»æœ«å°¾æå–)")
                            return result
                        except:
                            pass # Continue to Strategy 3
                        break # Found the matching brace but failed to parse, stop this strategy

        # Strategy 3: Find first { and matching } (Original/Common case)
        brace_start = text.find('{')
        if brace_start != -1:
            brace_count = 0
            json_end = -1
            for i in range(brace_start, len(text)):
                if text[i] == '{':
                    brace_count += 1
                elif text[i] == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        json_end = i
                        break
            
            if json_end != -1:
                json_text = text[brace_start:json_end+1]
                json_text = self._sanitize_json_text(json_text)
                try:
                    result = json.loads(json_text)
                    print(f"âœ“ JSONè§£ææˆåŠŸ (ç›´æ¥æå–)")
                    return result
                except json.JSONDecodeError:
                    pass  # Try other methods
        
        # If direct extraction failed, try to find JSON code block
        json_start = text.find('```json')
        if json_start != -1:
            json_content = text[json_start + 7:]
            json_end = json_content.find('```')
            if json_end != -1:
                text = json_content[:json_end].strip()
            else:
                text = json_content.strip()
        elif text.find('```') != -1:
            first_block = text.find('```')
            json_content = text[first_block + 3:]
            json_end = json_content.find('```')
            if json_end != -1:
                text = json_content[:json_end].strip()
        
        # Try to parse again with sanitization
        text = text.strip()
        if text.startswith('{'):
            text = self._sanitize_json_text(text)
            try:
                result = json.loads(text)
                print(f"âœ“ JSONè§£ææˆåŠŸ")
                return result
            except json.JSONDecodeError as e:
                print(f"âœ— JSONè§£æå¤±è´¥: {str(e)}")
                # Show more context for debugging
                print(f"æå–çš„JSONæ–‡æœ¬ï¼ˆå‰500å­—ç¬¦ï¼‰:\n{text[:500]}")
                print(f"åŸå§‹å“åº”ï¼ˆå‰500å­—ç¬¦ï¼‰:\n{response_text[:500]}")
                raise ValueError(f"Geminiè¿”å›çš„JSONæ— æ•ˆ: {str(e)}")
        
        raise ValueError(f"æ— æ³•ä»å“åº”ä¸­æå–JSONã€‚å“åº”å¼€å¤´: {response_text[:200]}")
