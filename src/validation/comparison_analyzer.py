"""
å¯¹æ¯”åˆ†ææ¨¡å—ï¼šç”Ÿæˆè§£æç»“æœä¸åŸå§‹æ–‡ä»¶çš„å…¨é¢å¯¹æ¯”æŠ¥å‘Šã€‚
"""
import json
import sys
import io
from pathlib import Path
from typing import Dict, List, Any, Optional
from collections import Counter
import difflib

# Handle numpy types for JSON serialization
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

# Fix encoding for Windows console
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False

from src.validation.pdf_comparator import PDFComparator


class ComparisonAnalyzer:
    """å¯¹æ¯”åˆ†æå™¨ï¼šç”Ÿæˆå…¨é¢çš„å¯¹æ¯”æŠ¥å‘Š"""
    
    def __init__(self):
        self.pdf_comparator = PDFComparator() if PYMUPDF_AVAILABLE else None
    
    def generate_comparison_report(
        self,
        original_pdf_path: str,
        structured_json_path: str,
        reconstructed_pdf_path: Optional[str] = None,
        validation_report_path: Optional[str] = None,
        output_dir: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå…¨é¢çš„å¯¹æ¯”åˆ†ææŠ¥å‘Šã€‚
        
        Args:
            original_pdf_path: åŸå§‹PDFè·¯å¾„ï¼ˆå¿…éœ€ï¼‰
            structured_json_path: ç»“æ„åŒ–JSONè·¯å¾„ï¼ˆå¿…éœ€ï¼‰
            reconstructed_pdf_path: é‡å»ºçš„PDFè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            validation_report_path: éªŒè¯æŠ¥å‘Šè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¯¹æ¯”æŠ¥å‘Šå­—å…¸
        """
        report = {
            "original_pdf": str(original_pdf_path),
            "structured_json": str(structured_json_path),
            "reconstructed_pdf": str(reconstructed_pdf_path) if reconstructed_pdf_path else None,
            "validation_report": str(validation_report_path) if validation_report_path else None,
        }
        
        print("\n" + "="*80)
        print("ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š")
        print("="*80)
        
        # 1. æ–‡æœ¬å¯¹æ¯”
        print("\n[1/5] æ–‡æœ¬å†…å®¹å¯¹æ¯”åˆ†æ...")
        print("-" * 80)
        text_comparison = self._compare_text_content(original_pdf_path, structured_json_path)
        if text_comparison:
            print(f"æ•´ä½“æ–‡æœ¬è¦†ç›–ç‡: {text_comparison.get('overall_coverage', 0):.2f}%")
            print(f"å¹³å‡é¡µé¢ç›¸ä¼¼åº¦: {text_comparison.get('avg_similarity', 0):.2f}%")
            report["text_comparison"] = text_comparison
        else:
            print("  è­¦å‘Š: æ— æ³•è¿›è¡Œæ–‡æœ¬å¯¹æ¯”")
        
        # 2. äº¤æ˜“æ•°æ®å¯¹æ¯”
        print("\n\n[2/5] äº¤æ˜“æ•°æ®å¯¹æ¯”åˆ†æ...")
        print("-" * 80)
        transactions_analysis = self._analyze_transactions(structured_json_path)
        if transactions_analysis:
            print(f"æ€»äº¤æ˜“æ•°: {transactions_analysis['total_transactions']}")
            print(f"åŒ…å«æ—¥æœŸ: {transactions_analysis['with_date']} ({transactions_analysis['date_coverage']:.2f}%)")
            print(f"åŒ…å«é‡‘é¢: {transactions_analysis['with_amount']} ({transactions_analysis['amount_coverage']:.2f}%)")
            print(f"åŒ…å«ä½™é¢: {transactions_analysis['with_balance']} ({transactions_analysis['balance_coverage']:.2f}%)")
            print(f"åŒ…å«æè¿°: {transactions_analysis['with_description']} ({transactions_analysis['description_coverage']:.2f}%)")
            report["transactions_analysis"] = transactions_analysis
        else:
            print("  è­¦å‘Š: æ— æ³•åˆ†æäº¤æ˜“æ•°æ®")
        
        # 3. å¸ƒå±€å…ƒç´ åˆ†æ
        print("\n\n[3/5] å¸ƒå±€å…ƒç´ åˆ†æ...")
        print("-" * 80)
        layout_analysis = self._analyze_layout_elements(structured_json_path)
        if layout_analysis:
            print(f"æ€»å…ƒç´ æ•°: {layout_analysis['total_elements']}")
            print(f"å…ƒç´ ç±»å‹åˆ†å¸ƒ:")
            for elem_type, count in layout_analysis['element_types'].items():
                print(f"  {elem_type}: {count}")
            print(f"åŒ…å«è¾¹ç•Œæ¡†çš„å…ƒç´ : {layout_analysis['elements_with_bbox']} ({layout_analysis['bbox_coverage']:.2f}%)")
            report["layout_analysis"] = layout_analysis
        else:
            print("  è­¦å‘Š: æ— æ³•åˆ†æå¸ƒå±€å…ƒç´ ")
        
        # 4. åƒç´ çº§å¯¹æ¯”ï¼ˆå¦‚æœæœ‰é‡å»ºçš„PDFï¼‰
        if reconstructed_pdf_path and Path(reconstructed_pdf_path).exists():
            print("\n\n[4/5] PDFåƒç´ çº§å¯¹æ¯”åˆ†æ...")
            print("-" * 80)
            pixel_comparison = self._compare_pdfs_pixel_level(original_pdf_path, reconstructed_pdf_path)
            if pixel_comparison and "error" not in pixel_comparison:
                print(f"æ€»ä½“åƒç´ å‡†ç¡®åº¦: {pixel_comparison.get('pixel_accuracy', 0):.2f}%")
                print(f"æ€»é¡µæ•°: {pixel_comparison.get('total_pages', 0)}")
                report["pixel_comparison"] = pixel_comparison
            else:
                print(f"  è­¦å‘Š: æ— æ³•è¿›è¡Œåƒç´ çº§å¯¹æ¯”")
                report["pixel_comparison"] = pixel_comparison or {"error": "Comparison failed"}
        else:
            print("\n\n[4/5] PDFåƒç´ çº§å¯¹æ¯”åˆ†æ...")
            print("-" * 80)
            print("  è·³è¿‡: æœªæ‰¾åˆ°é‡å»ºçš„PDFæ–‡ä»¶")
            report["pixel_comparison"] = {"skipped": True, "reason": "Reconstructed PDF not found"}
        
        # 5. éªŒè¯æŠ¥å‘Šåˆ†æ
        if validation_report_path and Path(validation_report_path).exists():
            print("\n\n[5/5] éªŒè¯æŠ¥å‘Šåˆ†æ...")
            print("-" * 80)
            validation_data = self._analyze_validation_report(validation_report_path)
            if validation_data:
                print(f"åƒç´ å‡†ç¡®åº¦: {validation_data.get('pixel_accuracy', 0):.2f}%")
                print(f"è¯­ä¹‰å‡†ç¡®åº¦: {validation_data.get('semantic_accuracy', 0):.2f}%")
                print(f"å·®å¼‚æ•°é‡: {len(validation_data.get('discrepancies', []))}")
                report["validation_report"] = validation_data
        else:
            print("\n\n[5/5] éªŒè¯æŠ¥å‘Šåˆ†æ...")
            print("-" * 80)
            print("  è·³è¿‡: æœªæ‰¾åˆ°éªŒè¯æŠ¥å‘Šæ–‡ä»¶")
            report["validation_report"] = {"skipped": True}
        
        # ç”Ÿæˆæ€»ç»“
        print("\n\n" + "=" * 80)
        print("å¯¹æ¯”åˆ†ææ€»ç»“")
        print("=" * 80)
        summary_items = []
        
        if report.get("text_comparison") and "overall_coverage" in report["text_comparison"]:
            coverage = report["text_comparison"]["overall_coverage"]
            summary_items.append(f"æ–‡æœ¬è¦†ç›–ç‡: {coverage:.2f}%")
        
        if report.get("transactions_analysis"):
            trans_total = report["transactions_analysis"]["total_transactions"]
            summary_items.append(f"æå–äº¤æ˜“æ•°: {trans_total}")
            balance_cov = report["transactions_analysis"]["balance_coverage"]
            summary_items.append(f"ä½™é¢å­—æ®µå®Œæ•´æ€§: {balance_cov:.2f}%")
        
        if report.get("pixel_comparison") and "pixel_accuracy" in report["pixel_comparison"]:
            pixel_acc = report["pixel_comparison"]["pixel_accuracy"]
            summary_items.append(f"åƒç´ å‡†ç¡®åº¦: {pixel_acc:.2f}%")
        
        if report.get("validation_report") and "semantic_accuracy" in report["validation_report"]:
            semantic_acc = report["validation_report"]["semantic_accuracy"]
            summary_items.append(f"è¯­ä¹‰å‡†ç¡®åº¦: {semantic_acc:.2f}%")
        
        for item in summary_items:
            print(f"  {item}")
        
        # ä¿å­˜æŠ¥å‘Š
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜JSONæŠ¥å‘Š
            json_report_path = output_path / "comparison_report.json"
            try:
                with open(json_report_path, 'w', encoding='utf-8') as f:
                    json.dump(report, f, ensure_ascii=False, indent=2, default=self._json_serializer)
                print(f"\nè¯¦ç»†å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜è‡³: {json_report_path}")
            except Exception as e:
                print(f"Warning: Failed to save JSON report: {e}")
                import traceback
                traceback.print_exc()
            
            # ä¿å­˜MarkdownæŠ¥å‘Š
            md_report_path = output_path / "comparison_report.md"
            try:
                self._save_markdown_report(report, md_report_path)
                if md_report_path.exists():
                    print(f"MarkdownæŠ¥å‘Šå·²ä¿å­˜è‡³: {md_report_path}")
                else:
                    print(f"Warning: MarkdownæŠ¥å‘Šä¿å­˜å¤±è´¥ï¼Œæ–‡ä»¶æœªåˆ›å»º")
            except Exception as e:
                print(f"Warning: Failed to save Markdown report: {e}")
                import traceback
                traceback.print_exc()
        
        print("=" * 80)
        return report
    
    def _compare_text_content(self, original_pdf: str, structured_json: str) -> Optional[Dict[str, Any]]:
        """å¯¹æ¯”æ–‡æœ¬å†…å®¹"""
        try:
            original_texts = self._extract_text_from_pdf(original_pdf)
            extracted_texts = self._extract_text_from_structured_data(structured_json)
            
            if not original_texts or not extracted_texts:
                return None
            
            page_comparisons = []
            total_chars_original = 0
            total_chars_extracted = 0
            total_matching_chars = 0
            
            for page_num in sorted(set(list(original_texts.keys()) + list(extracted_texts.keys()))):
                orig_text = original_texts.get(page_num, "")
                extr_text = extracted_texts.get(page_num, "")
                
                orig_normalized = self._normalize_text(orig_text)
                extr_normalized = self._normalize_text(extr_text)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = difflib.SequenceMatcher(None, orig_normalized, extr_normalized).ratio() * 100
                
                # è®¡ç®—å­—ç¬¦è¦†ç›–ç‡
                orig_chars = len(orig_normalized)
                extr_chars = len(extr_normalized)
                matching_chars = sum(1 for c in orig_normalized if c in extr_normalized)
                
                coverage = (matching_chars / orig_chars * 100) if orig_chars > 0 else 0
                
                total_chars_original += orig_chars
                total_chars_extracted += extr_chars
                total_matching_chars += matching_chars
                
                page_comparisons.append({
                    "page": page_num,
                    "original_chars": orig_chars,
                    "extracted_chars": extr_chars,
                    "similarity": similarity,
                    "coverage": coverage
                })
            
            overall_coverage = (total_matching_chars / total_chars_original * 100) if total_chars_original > 0 else 0
            avg_similarity = sum(p["similarity"] for p in page_comparisons) / len(page_comparisons) if page_comparisons else 0
            
            return {
                "overall_coverage": overall_coverage,
                "avg_similarity": avg_similarity,
                "total_chars_original": total_chars_original,
                "total_chars_extracted": total_chars_extracted,
                "page_comparisons": page_comparisons
            }
        except Exception as e:
            print(f"  é”™è¯¯: {e}")
            return None
    
    def _analyze_transactions(self, structured_json: str) -> Optional[Dict[str, Any]]:
        """åˆ†æäº¤æ˜“æ•°æ®"""
        try:
            with open(structured_json, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            transactions = data.get("structured_data", {}).get("account_summary", {}).get("transactions", [])
            
            if not transactions:
                return None
            
            total = len(transactions)
            with_date = sum(1 for t in transactions if t.get("date"))
            with_amount = sum(1 for t in transactions if t.get("amount") is not None)
            with_balance = sum(1 for t in transactions if t.get("balance") is not None)
            with_description = sum(1 for t in transactions if t.get("description"))
            
            return {
                "total_transactions": total,
                "with_date": with_date,
                "date_coverage": (with_date / total * 100) if total > 0 else 0,
                "with_amount": with_amount,
                "amount_coverage": (with_amount / total * 100) if total > 0 else 0,
                "with_balance": with_balance,
                "balance_coverage": (with_balance / total * 100) if total > 0 else 0,
                "with_description": with_description,
                "description_coverage": (with_description / total * 100) if total > 0 else 0,
            }
        except Exception as e:
            print(f"  é”™è¯¯: {e}")
            return None
    
    def _analyze_layout_elements(self, structured_json: str) -> Optional[Dict[str, Any]]:
        """åˆ†æå¸ƒå±€å…ƒç´ """
        try:
            with open(structured_json, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            total_elements = 0
            elements_with_bbox = 0
            element_types = Counter()
            
            for page_data in data.get("pages", []):
                for element in page_data.get("layout_elements", []):
                    total_elements += 1
                    elem_type = element.get("type", "unknown")
                    element_types[elem_type] += 1
                    
                    bbox = element.get("bbox")
                    if bbox and isinstance(bbox, dict):
                        if bbox.get("width", 0) > 0 and bbox.get("height", 0) > 0:
                            elements_with_bbox += 1
            
            bbox_coverage = (elements_with_bbox / total_elements * 100) if total_elements > 0 else 0
            
            return {
                "total_elements": total_elements,
                "elements_with_bbox": elements_with_bbox,
                "bbox_coverage": bbox_coverage,
                "element_types": dict(element_types)
            }
        except Exception as e:
            print(f"  é”™è¯¯: {e}")
            return None
    
    def _compare_pdfs_pixel_level(self, original_pdf: str, reconstructed_pdf: str) -> Optional[Dict[str, Any]]:
        """åƒç´ çº§å¯¹æ¯”"""
        if not self.pdf_comparator:
            return {"error": "PDFComparator not available"}
        
        try:
            result = self.pdf_comparator.compare_pdfs(original_pdf, reconstructed_pdf)
            # Convert numpy types to Python types for JSON serialization
            if isinstance(result, dict):
                if "is_valid" in result:
                    result["is_valid"] = bool(result["is_valid"])
                # Convert any numpy types in nested structures
                result = self._convert_numpy_types(result)
            return result
        except Exception as e:
            return {"error": str(e)}
    
    def _convert_numpy_types(self, obj):
        """é€’å½’è½¬æ¢å­—å…¸/åˆ—è¡¨ä¸­çš„numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
        if HAS_NUMPY:
            if isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
        
        if isinstance(obj, dict):
            return {k: self._convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        else:
            return obj
    
    def _analyze_validation_report(self, validation_report_path: str) -> Optional[Dict[str, Any]]:
        """åˆ†æéªŒè¯æŠ¥å‘Š"""
        try:
            with open(validation_report_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"  é”™è¯¯: {e}")
            return None
    
    def _extract_text_from_pdf(self, pdf_path: str) -> Dict[int, str]:
        """ä»PDFæå–æ–‡æœ¬"""
        if not PYMUPDF_AVAILABLE:
            return {}
        
        try:
            doc = fitz.open(str(pdf_path))
            pages_text = {}
            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text("text")
                pages_text[page_num + 1] = text
            doc.close()
            return pages_text
        except Exception as e:
            print(f"  é”™è¯¯æå–PDFæ–‡æœ¬: {e}")
            return {}
    
    def _extract_text_from_structured_data(self, json_path: str) -> Dict[int, str]:
        """ä»ç»“æ„åŒ–æ•°æ®æå–æ–‡æœ¬"""
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            pages_text = {}
            for page_data in data.get("pages", []):
                page_num = page_data.get("page_number", 1)
                text_parts = []
                
                for element in page_data.get("layout_elements", []):
                    if element.get("type") == "text":
                        content = element.get("content", "")
                        if content:
                            text_parts.append(str(content))
                    elif element.get("type") == "table":
                        table_content = element.get("content", {})
                        if isinstance(table_content, dict):
                            rows = table_content.get("rows", [])
                            for row in rows:
                                if isinstance(row, list):
                                    text_parts.append(" | ".join(str(cell) for cell in row))
                                elif isinstance(row, dict):
                                    text_parts.append(" | ".join(str(v) for v in row.values()))
                
                pages_text[page_num] = "\n".join(text_parts)
            return pages_text
        except Exception as e:
            print(f"  é”™è¯¯æå–ç»“æ„åŒ–æ•°æ®æ–‡æœ¬: {e}")
            return {}
    
    def _normalize_text(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ–‡æœ¬"""
        if not text:
            return ""
        lines = text.split('\n')
        normalized_lines = [line.strip() for line in lines if line.strip()]
        return ' '.join(normalized_lines)
    
    def _json_serializer(self, obj):
        """JSONåºåˆ—åŒ–è¾…åŠ©å‡½æ•°ï¼Œå¤„ç†numpyç±»å‹å’Œå…¶ä»–ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡"""
        if HAS_NUMPY:
            if isinstance(obj, (np.integer, np.int64, np.int32)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64, np.float32)):
                return float(obj)
            elif isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
        # Handle other types
        if isinstance(obj, (set, frozenset)):
            return list(obj)
        # Default: convert to string
        return str(obj)
    
    def _save_markdown_report(self, report: Dict[str, Any], output_path: Path):
        """ä¿å­˜Markdownæ ¼å¼çš„æŠ¥å‘Š"""
        md_content = ["# è§£æç»“æœå¯¹æ¯”åˆ†ææŠ¥å‘Š\n"]
        
        # åŸºæœ¬ä¿¡æ¯
        md_content.append("## ğŸ“‹ åŸºæœ¬ä¿¡æ¯\n")
        md_content.append(f"- **åŸå§‹PDF**: {report['original_pdf']}\n")
        md_content.append(f"- **ç»“æ„åŒ–JSON**: {report['structured_json']}\n")
        if report.get('reconstructed_pdf'):
            md_content.append(f"- **é‡å»ºPDF**: {report['reconstructed_pdf']}\n")
        md_content.append("\n")
        
        # æ–‡æœ¬å¯¹æ¯”
        if report.get("text_comparison"):
            tc = report["text_comparison"]
            md_content.append("## ğŸ“„ æ–‡æœ¬å†…å®¹å¯¹æ¯”\n")
            md_content.append(f"- **æ•´ä½“æ–‡æœ¬è¦†ç›–ç‡**: {tc.get('overall_coverage', 0):.2f}%\n")
            md_content.append(f"- **å¹³å‡é¡µé¢ç›¸ä¼¼åº¦**: {tc.get('avg_similarity', 0):.2f}%\n")
            md_content.append("\n")
        
        # äº¤æ˜“æ•°æ®
        if report.get("transactions_analysis"):
            ta = report["transactions_analysis"]
            md_content.append("## ğŸ’° äº¤æ˜“æ•°æ®å¯¹æ¯”\n")
            md_content.append(f"- **æ€»äº¤æ˜“æ•°**: {ta['total_transactions']}\n")
            md_content.append(f"- **æ—¥æœŸå­—æ®µå®Œæ•´æ€§**: {ta['date_coverage']:.2f}%\n")
            md_content.append(f"- **é‡‘é¢å­—æ®µå®Œæ•´æ€§**: {ta['amount_coverage']:.2f}%\n")
            md_content.append(f"- **ä½™é¢å­—æ®µå®Œæ•´æ€§**: {ta['balance_coverage']:.2f}%\n")
            md_content.append(f"- **æè¿°å­—æ®µå®Œæ•´æ€§**: {ta['description_coverage']:.2f}%\n")
            md_content.append("\n")
        
        # å¸ƒå±€å…ƒç´ 
        if report.get("layout_analysis"):
            la = report["layout_analysis"]
            md_content.append("## ğŸ¨ å¸ƒå±€å…ƒç´ åˆ†æ\n")
            md_content.append(f"- **æ€»å…ƒç´ æ•°**: {la['total_elements']}\n")
            md_content.append(f"- **è¾¹ç•Œæ¡†è¦†ç›–ç‡**: {la['bbox_coverage']:.2f}%\n")
            md_content.append("\n**å…ƒç´ ç±»å‹åˆ†å¸ƒ**:\n")
            for elem_type, count in la['element_types'].items():
                md_content.append(f"- {elem_type}: {count}\n")
            md_content.append("\n")
        
        # åƒç´ å¯¹æ¯”
        if report.get("pixel_comparison") and "pixel_accuracy" in report["pixel_comparison"]:
            pc = report["pixel_comparison"]
            md_content.append("## ğŸ–¼ï¸ åƒç´ çº§å¯¹æ¯”\n")
            md_content.append(f"- **æ€»ä½“åƒç´ å‡†ç¡®åº¦**: {pc.get('pixel_accuracy', 0):.2f}%\n")
            md_content.append("\n")
        
        # éªŒè¯æŠ¥å‘Š
        if report.get("validation_report") and "semantic_accuracy" in report["validation_report"]:
            vr = report["validation_report"]
            md_content.append("## âœ… éªŒè¯æŠ¥å‘Š\n")
            md_content.append(f"- **è¯­ä¹‰å‡†ç¡®åº¦**: {vr.get('semantic_accuracy', 0):.2f}%\n")
            md_content.append(f"- **å·®å¼‚æ•°é‡**: {len(vr.get('discrepancies', []))}\n")
            md_content.append("\n")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(''.join(md_content))

